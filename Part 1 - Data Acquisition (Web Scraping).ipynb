{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Curation (Web Scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set wait times\n",
    "waittime = 30\n",
    "sleeptime = 0.5\n",
    "\n",
    "# Initiate web driver\n",
    "try:\n",
    "    driver.close() # Close any existing WebDrivers\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Define target website\n",
    "home_page = \"https://www.99.co/singapore/rent/condos-apartments\"\n",
    "\n",
    "# Set webdriver options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('ignore-certificate-errors')\n",
    "\n",
    "# Initiate webdriver\n",
    "driver = webdriver.Chrome(options=options) \n",
    "\n",
    "# Get driver to retrieve homepage\n",
    "driver.get(home_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last page number (of all listings)\n",
    "def get_last_page_number():\n",
    "    pagination_elems = driver.find_elements_by_class_name(\"SearchPagination\") \n",
    "    page_numbers = [elem.text.split(\"\\n\") for elem in pagination_elems][0]\n",
    "    last_page = int(page_numbers[-2])\n",
    "    return last_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate all web links on current listings page\n",
    "def collate_web_links():\n",
    "    elems = driver.find_elements_by_css_selector(\"._31ajL [href]\") # Div class just above div style=\"opacity:1\"\n",
    "    links = [elem.get_attribute('href') for elem in elems] # Get the web links present on current page\n",
    "    \n",
    "    substring = '/singapore/rent/property/'\n",
    "    # Truncate web links to remove unnecessary last part of string\n",
    "    condo_links = [link.split(\"?\", 1)[0] for link in links if substring in link] # List comprehension ensuring link is directed to property page, not ad\n",
    "    \n",
    "    return condo_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape detailed page (Old web template)\n",
    "# Example of old version: https://www.99.co/singapore/rent/property/parc-sophia-condo-kcqL4yS7E38TEc29bHwNrY\n",
    "\n",
    "def scrape_page_old():\n",
    "    details_dict = {}\n",
    "\n",
    "    # Scrape info on page\n",
    "    class_name_1 = 'Z0npN'\n",
    "    details_dict['title'] = driver.find_element_by_xpath(f\"//h1[@class='{class_name_1}']\").text\n",
    "    details_dict['rental'] = driver.find_elements_by_xpath(f\"//div[@id='price']/h3[@class='{class_name_1}']\")[0].text\n",
    "\n",
    "    # Property details\n",
    "    elems = driver.find_elements_by_xpath(f\"//p[@class='JPolj _5q1E-']\")    \n",
    "    details = [elem.text.split(\" \") for elem in elems]\n",
    "    for sub_list in details:\n",
    "        if len(sub_list) == 1:\n",
    "            details_dict[f\"{sub_list[0].lower()}\"] = sub_list[0].lower() # This is to cater to Studio label\n",
    "        else:\n",
    "            details_dict[f\"{sub_list[1].lower()}\"] = sub_list[0] # Catering to the other property details e.g. no. of baths\n",
    "\n",
    "    class_name_title = 'FuuOS'\n",
    "    class_name_div = '_28kbc'\n",
    "    class_name_category = '_24Agy'\n",
    "    class_name_value = 'JPolj'\n",
    "\n",
    "    num_of_property_details = len(driver.find_elements_by_xpath(f\"//div[@class='{class_name_category}']\"))\n",
    "    for i in range(num_of_property_details):\n",
    "        detail_category = driver.find_elements_by_xpath(f\"//div[@class='{class_name_category}']\")[i].text\n",
    "        detail_category = detail_category.lower()\n",
    "        detail_category = detail_category.replace(\" \",\"_\")\n",
    "        details_dict[f'{detail_category}'] = driver.find_elements_by_xpath(f\"//div[@class='{class_name_div}']/p[@class='{class_name_value}']\")[i].text\n",
    "\n",
    "        \n",
    "    # Development details\n",
    "    dev_details = driver.find_elements_by_xpath(f\"//div[@class='_2U7f7']/div/p[@class='JPolj']\")\n",
    "    for detail in dev_details:\n",
    "        detail_text = detail.text.split(\": \")\n",
    "        detail_category = detail_text[0].lower()\n",
    "        detail_category = detail_category.replace(\" \",\"_\")\n",
    "        details_dict[f'{detail_category}'] = detail_text[1]     \n",
    "        \n",
    "    # MRT\n",
    "    try:\n",
    "        details_dict['nearest_mrt_name'] = driver.find_element_by_xpath(f\"//a[@class='_2aXf0 QE8xc']\").text\n",
    "        details_dict['nearest_mrt_dist'] = driver.find_element_by_xpath(f\"//p[@class='JPolj _2ZoIs']\").text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Amenities\n",
    "    amenities_elems = driver.find_elements_by_xpath(\"//p[@class='JPolj _26v8n']\")\n",
    "    details_dict['amenities'] = [str(elem.text) for elem in amenities_elems]\n",
    "\n",
    "    details_dict['electoral_div'] = driver.find_elements_by_xpath(f\"//h2[@class='Z0npN _3NW6g']\")[0].text\n",
    "\n",
    "    map_element = driver.find_element_by_id(\"location\")\n",
    "    actions = ActionChains(driver)\n",
    "    actions.move_to_element(map_element).perform()\n",
    "\n",
    "    class_name_location_div = '_3OnRG'\n",
    "    class_name_location = 'yMCxv _1YwzE _1vzK2'\n",
    "    details_dict['travel_time_changi'] = WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, f\"//div[@class='{class_name_location_div}']/h4[@class='{class_name_location}']\"))).text\n",
    "    details_dict['travel_time_raffles'] = driver.find_elements_by_xpath(f\"//div[@class='{class_name_location_div}']/h4[@class='{class_name_location}']\")[1].text\n",
    "    details_dict['travel_time_orchard'] = driver.find_elements_by_xpath(f\"//div[@class='{class_name_location_div}']/h4[@class='{class_name_location}']\")[2].text\n",
    "    \n",
    "    return details_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape detailed page (New Web Template)\n",
    "def scrape_page_new():\n",
    "        \n",
    "    details_dict = {}\n",
    "\n",
    "    # Scrape info on page\n",
    "    class_name_1 = '_1zGm8 _1vzK2'\n",
    "    WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, f\"//h1[@class='{class_name_1}']\")))\n",
    "    details_dict['title'] = driver.find_element_by_xpath(f\"//h1[@class='{class_name_1}']\").text #Updated\n",
    "    details_dict['rental'] = driver.find_elements_by_xpath(f\"//h2[@class='{class_name_1}']\")[0].text #Updated\n",
    "    details_dict['bed'] = driver.find_elements_by_xpath(f\"//h2[@class='{class_name_1}']\")[1].text #Updated\n",
    "    details_dict['bath'] = driver.find_elements_by_xpath(f\"//h2[@class='{class_name_1}']\")[2].text #Updated\n",
    "    details_dict['sqft'] = driver.find_elements_by_xpath(f\"//h2[@class='{class_name_1}']\")[3].text #Updated\n",
    "\n",
    "    class_name_2 = 'dniCg _2rhE-'\n",
    "    details_dict['psf'] = driver.find_elements_by_xpath(f\"//p[@class='{class_name_2}']\")[1].text \n",
    "\n",
    "    # This section will take into account these details (if present): Availability, Lease, Furnishing, Property Type, \n",
    "    # Name, Unit Types, Total Units, Built Year, Tenure, Developer, and Neighbourhood\n",
    "    class_name_td1 = '_3NChA'\n",
    "    class_name_td2 = 'dm2g6'\n",
    "    num_of_property_details = len(driver.find_elements_by_xpath(f\"//td[contains(@class, '{class_name_td1}')]\"))\n",
    "    for i in range(num_of_property_details):\n",
    "        detail_category = driver.find_elements_by_xpath(f\"//td[contains(@class, '{class_name_td1}')]\")[i].text\n",
    "        detail_category = detail_category.lower()\n",
    "        detail_category = detail_category.replace(\" \",\"_\")\n",
    "        details_dict[f'{detail_category}'] = driver.find_elements_by_xpath(f\"//td[@class='{class_name_td2}']\")[i].text\n",
    "\n",
    "    # MRT\n",
    "    try:\n",
    "        details_dict['nearest_mrt_name'] = driver.find_element_by_xpath(f\"//a[@class='_2aXf0 QE8xc']\").text\n",
    "        details_dict['nearest_mrt_dist'] = driver.find_element_by_xpath(f\"//p[@class='JPolj _2ZoIs']\").text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Expand amenities\n",
    "    button_class = 'cFGt2 _1P_YF' # For amenities\n",
    "    WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, f\"//button[@class='{button_class}']\")))\n",
    "    driver.find_element_by_xpath(f\"//button[@class='{button_class}']\").click()\n",
    "    time.sleep(sleeptime)\n",
    "\n",
    "    # Extract all amenities \n",
    "    amenities_elems = driver.find_elements_by_xpath(f\"//p[@class='_2sIc2 AIgs2 _2rhE-']\")\n",
    "    details_dict['amenities'] = [str(elem.text) for elem in amenities_elems]\n",
    "\n",
    "    details_dict['electoral_div'] = driver.find_elements_by_xpath(f\"//h2[@class='Z0npN _3NW6g']\")[0].text\n",
    "\n",
    "    # Scroll down to reveal Google map section\n",
    "    map_element = driver.find_element_by_xpath(\"//div[@class='z3BrQ']\")\n",
    "    actions = ActionChains(driver)\n",
    "    actions.move_to_element(map_element).perform()\n",
    "    \n",
    "    class_name_location_div = '_3OnRG'\n",
    "    class_name_location = 'yMCxv _1YwzE _1vzK2'\n",
    "    \n",
    "    time.sleep(sleeptime)\n",
    "    details_dict['travel_time_changi'] = WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, f\"//div[@class='{class_name_location_div}']/h4[@class='{class_name_location}']\"))).text\n",
    "    details_dict['travel_time_raffles'] = driver.find_elements_by_xpath(f\"//div[@class='{class_name_location_div}']/h4[@class='{class_name_location}']\")[1].text\n",
    "    details_dict['travel_time_orchard'] = driver.find_elements_by_xpath(f\"//div[@class='{class_name_location_div}']/h4[@class='{class_name_location}']\")[2].text\n",
    "    \n",
    "    return details_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open web link and scrape information on that page\n",
    "def open_and_scrape(web_link, master_list_of_dict):\n",
    "        \n",
    "    # Switch to web link\n",
    "    driver.get(web_link)\n",
    "    \n",
    "    # Scrape page based on whether old web design or new design\n",
    "    # Old version has the class name \"_2yeD-\" just below the <div id = appContent>\n",
    "    if len(driver.find_elements_by_class_name(\"_2yeD-\"))!=0:\n",
    "        details_dict = scrape_page_old() \n",
    "         \n",
    "    else:\n",
    "        details_dict = scrape_page_new()\n",
    "        \n",
    "    master_list_of_dict.append(details_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the dictionaries (each listing should give one dictionary)\n",
    "master_list_of_dict = []\n",
    "\n",
    "# Get the last page number in terms of pages of available listings\n",
    "last_page_num = get_last_page_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_scraping(start_page):\n",
    "        \n",
    "    for i in range(1, last_page_num+1):\n",
    "        current_page = WebDriverWait(driver, waittime).until(EC.presence_of_element_located((By.XPATH, \"//li[@class='active']\"))).text\n",
    "        current_page = int(current_page)\n",
    "\n",
    "        if current_page < last_page_num:\n",
    "            condo_links = collate_web_links()\n",
    "            for index,link in enumerate(condo_links):\n",
    "                try:\n",
    "                    open_and_scrape(link, master_list_of_dict)\n",
    "                except:\n",
    "                    pass\n",
    "                print(f\"Completed {index+1} out of {len(condo_links)} links of Page {start_page}\")\n",
    "            start_page += 1\n",
    "            try:\n",
    "                next_link = f'{home_page}?page_num={start_page}'\n",
    "                driver.get(next_link)\n",
    "            except:\n",
    "                time.sleep(10)\n",
    "                next_link = f'{home_page}?page_num={start_page}'\n",
    "                driver.get(next_link)\n",
    "\n",
    "        else:\n",
    "            condo_links = collate_web_links()\n",
    "            for link in condo_links:\n",
    "                open_and_scrape(link, master_list_of_dict)\n",
    "            print('Scraping complete')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1 out of 35 links of Page 1\n",
      "Completed 2 out of 35 links of Page 1\n",
      "Completed 3 out of 35 links of Page 1\n",
      "Completed 4 out of 35 links of Page 1\n",
      "Completed 5 out of 35 links of Page 1\n"
     ]
    }
   ],
   "source": [
    "execute_scraping(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7958"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of dictionaries in the list\n",
    "len(master_list_of_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to remove duplicate entries in the list of dictionaries\n",
    "def remove_dupe_dicts(l):\n",
    "    list_of_strings = [\n",
    "    json.dumps(d, sort_keys=True)\n",
    "    for d in l]\n",
    "\n",
    "    list_of_strings = set(list_of_strings)\n",
    "\n",
    "    return [json.loads(s) for s in list_of_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = remove_dupe_dicts(master_list_of_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7317"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of listings successfully scraped\n",
    "len(master_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as JSON file\n",
    "with open('master_list_cleaned_v3.json', 'w') as file:\n",
    "    file.write(json.dumps(master_list, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
